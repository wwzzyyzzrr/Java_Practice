<html><head><meta charset="utf-8"></meta></head><body><h1 class="article-title" style="text-align:center;">Scrapy蜘蛛（Spider）</h1><div style="width:100%;float:left;" class="article-content">   
 <div> 
  <div>
    Spider是负责定义如何遵循通过网站的链接并提取网页中的信息的类。 
  </div> 
 </div> 
 <div> 
  <div>
    Scrapy默认的&nbsp;Spider&nbsp;如下： 
  </div> 
  <h3> scrapy.Spider </h3> 
  <div>
    它是所有其他的蜘蛛(spider)都必须继承的类。它具有以下类： 
  </div> 
  <pre>class scrapy.spiders.Spider</pre> 
  <div>
    下面的表显示了 scrapy.Spider 类的字段： 
  </div> 
  <table border="1" cellpadding="2" cellspacing="0"> 
   <tbody> 
    <tr> 
     <th style="border:1px solid #999999;"> S.N. </th> 
     <th style="border:1px solid #999999;"> 字段 &amp; 描述 </th> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 1 </td> 
     <td style="border:1px solid #999999;"> name<br> 这是&nbsp;spider&nbsp;的名字 </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 2 </td> 
     <td style="border:1px solid #999999;"> allowed_domains<br> 
      <div>
        它是允许&nbsp;spider&nbsp;抓取域名称的列表 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 3 </td> 
     <td style="border:1px solid #999999;"> start_urls<br> 
      <div>
        这是供以后蜘蛛将开始抓取的URL列表的根 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 4 </td> 
     <td style="border:1px solid #999999;"> custom_settings<br> 
      <div>
        这些设置在蜘蛛运行时会从项目范围内覆盖配置 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 5 </td> 
     <td style="border:1px solid #999999;"> crawler<br> 
      <div>
        它是链接到 spider 实例绑定的 Crawler 对象的属性 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 6 </td> 
     <td style="border:1px solid #999999;"> settings<br> 
      <div>
        这些是运行一个&nbsp;spider&nbsp;的设置 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 7 </td> 
     <td style="border:1px solid #999999;"> logger<br> 
      <div>
        它是用来发送日志消息的 python 记录器 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 8 </td> 
     <td style="border:1px solid #999999;"> from_crawler(crawler,*args,**kwargs)<br> 
      <div>
        它是由&nbsp;spider&nbsp;创建的一个类方法。参数是： 
      </div> 
      <ul> 
       <li> <p style="text-align:justify;"> crawler:&nbsp;抓取工具到&nbsp;spider&nbsp;实例将被绑定； </p> </li> 
       <li> <p style="text-align:justify;"> args(list):&nbsp;这些参数传递给方法：&nbsp;_init_()； </p> </li> 
       <li> <p style="text-align:justify;"> kwargs(dict):&nbsp;这些关键字参数传递给方法:&nbsp;_init_(). </p> </li> 
      </ul> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 9 </td> 
     <td style="border:1px solid #999999;"> start_requests()<br> 
      <div>
        如果不指定特定的URL，蜘蛛会打开抓取，Scrapy调用start_requests()方法 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 10 </td> 
     <td style="border:1px solid #999999;"> make_requests_from_url(url)<br> 
      <div>
        它是用于将URL网址转换为请求方法 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 11 </td> 
     <td style="border:1px solid #999999;"> parse(response)<br> 
      <div>
        这个方法处理响应并返回废弃数据 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 12 </td> 
     <td style="border:1px solid #999999;"> log(message[,level,component])<br> 
      <div>
        这个方法会通过蜘蛛发送日志记录信息 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 13 </td> 
     <td style="border:1px solid #999999;"> closed(reason)<br> 
      <div>
        这种方法在当蜘蛛关闭时调用 
      </div> </td> 
    </tr> 
   </tbody> 
  </table> 
  <h2> Spider参数 </h2> 
  <div>
    Spider&nbsp;参数用于指定起始URL和使用带有-a选项的抓取命令来传递，如下图所示： 
  </div> 
  <pre>scrapy crawl first_scrapy -a group = accessories</pre> 
  <div>
    下面的代码示例显示蜘蛛是如何接收参数的： 
  </div> 
  <pre>import scrapy

class FirstSpider(scrapy.Spider):
    name = "first"

    def __init__(self, group=None, *args, **kwargs):
        super(FirstSpider, self).__init__(*args, **kwargs)
        self.start_urls = ["http://www.yiibai.com/group/%s" % group]</pre> 
  <h2> 通用Spider </h2> 
  <p style="text-align:justify;"> 您可以使用通用蜘蛛来创建子类蜘蛛。他们的目的是要根据一定的规则来提取所有在网站上的所有链接的数据。 </p> 
  <div>
    例如： 
  </div> 
  <div>
    我们假设项目有以下的字段： 
  </div> 
  <pre>import scrapy
from scrapy.item import Item, Field
	
class First_scrapyItem(scrapy.Item):
    product_title = Field()
    product_link = Field()
    product_description = Field()</pre> 
  <h2> CrawlSpider </h2> 
  <div>
    CrawlSpider定义了一套规律可循的联系，并取消多个页面。它具有以下类： 
  </div> 
  <pre>class scrapy.spiders.CrawlSpider</pre> 
  <div>
    以下是CrawlSpider类的属性： 
  </div> 
  <h3> rules </h3> 
  <div>
    这是规则对象的列表，它定义了爬网程序如何抓取下面的链接。 
  </div> 
  <div>
    下面的表显示了CrawlSpider类的规则： 
  </div> 
  <table border="1" cellpadding="2" cellspacing="0"> 
   <tbody> 
    <tr> 
     <th style="border:1px solid #999999;"> S.N. </th> 
     <th style="border:1px solid #999999;"> 
      <div>
        规则和说明 
      </div> </th> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 1 </td> 
     <td style="border:1px solid #999999;"> LinkExtractor<br> 
      <div>
        它指定蜘蛛如何跟随链接和提取数据； 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 2 </td> 
     <td style="border:1px solid #999999;"> callback<br> 
      <div>
        它是在每一页提取之后被调用；&nbsp; 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 3 </td> 
     <td style="border:1px solid #999999;"> follow<br> 
      <div>
        它指定是否继续跟踪链接； 
      </div> </td> 
    </tr> 
   </tbody> 
  </table> 
  <h3> parse_start_url(response) </h3> 
  <p style="text-align:justify;"> 它通过允许解析初步回应返回项目或请求对象。 </p> 
  <p style="text-align:justify;"> 注意:&nbsp;请务必重命名等函数解析而不是编写规则，因为解析函数用于CrawlSpider来实现它的逻辑。 </p> 
  <div>
    例如： 
  </div> 
  <p style="text-align:justify;"> 让我们看看下面的例子开始演示蜘蛛爬行&nbsp;example.com 首页，使用&nbsp;parse_items 方法收集所有页面上的链接和词组： </p> 
  <pre>import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

class DemoSpider(CrawlSpider):
    name = "demo"
    allowed_domains = ["www.yiibai.com"]
    start_urls = ["http://www.yiibai.com"]

    rules = (
    Rule(LinkExtractor(allow =(), restrict_xpaths = ("//div[@class = 'next']",)), callback = "parse_item", follow = True),
    )

    def parse_item(self, response):
        item = DemoItem()
        item["product_title"] = response.xpath("a/text()").extract()
        item["product_link"] = response.xpath("a/@href").extract()
        item["product_description"]  = response.xpath("div[@class='desc']/text()").extract()
        return items</pre> 
  <h2> XMLFeedSpider </h2> 
  <div>
    它是从XML的Feed提取并遍历节点的蜘蛛的基类。它具有以下类： 
  </div> 
  <pre>class scrapy.spiders.XMLFeedSpider</pre> 
  <div>
    下表显示了用于设置iterator和标记名称的类属性： 
  </div> 
  <table border="1" cellpadding="2" cellspacing="0"> 
   <tbody> 
    <tr> 
     <th style="border:1px solid #999999;"> S.N. </th> 
     <th style="border:1px solid #999999;"> 
      <div>
        属性和说明 
      </div> </th> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 1 </td> 
     <td style="border:1px solid #999999;"> iterator<br> 
      <div>
        它定义将要使用的迭代器。它可以是iternodes，HTML或XML。默认为：iternodes 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 2 </td> 
     <td style="border:1px solid #999999;"> itertag<br> 
      <div>
        它使用节点名称的字符串进行迭代 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 3 </td> 
     <td style="border:1px solid #999999;"> namespaces<br> 
      <div>
        它是由(prefix,&nbsp;uri)元组使用register_namespace()方法自动注册命名空间的列表中定义 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 4 </td> 
     <td style="border:1px solid #999999;"> adapt_response(response)<br> 
      <div>
        它接收响应，并尽快在开始解析之前从蜘蛛中间件修改响应体 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 5 </td> 
     <td style="border:1px solid #999999;"> parse_node(response,selector)<br> 它接收到响应和选择器，在每个节点匹配提供标签名时调用<br> 
      <div>
        注意：如果不重写此方法，蜘蛛将不起作用 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 6 </td> 
     <td style="border:1px solid #999999;"> process_results(response,results)<br> 
      <div>
        它由蜘蛛返回结果和响应列表 
      </div> </td> 
    </tr> 
   </tbody> 
  </table> 
  <h2> CSVFeedSpider </h2> 
  <div>
    它通过它的每行的迭代，收到一个CSV文件作为响应，并调用&nbsp;parse_row()&nbsp;方法。它具有以下类： 
  </div>   
  <pre>class scrapy.spiders.CSVFeedSpider</pre> 
  <div>
    下表显示了可设置关于CSV文件的选项： 
  </div> 
  <table border="1" cellpadding="2" cellspacing="0"> 
   <tbody> 
    <tr> 
     <th style="border:1px solid #999999;"> S.N. </th> 
     <th style="border:1px solid #999999;"> 
      <div>
        选项及说明 
      </div> </th> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 1 </td> 
     <td style="border:1px solid #999999;"> delimiter<br> 
      <div>
        它是包含每个字段使用逗号(“,”)分隔的字符串 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 2 </td> 
     <td style="border:1px solid #999999;"> quotechar<br> 
      <div>
        这是一个包含每个字段使用引号('"')字符串 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 3 </td> 
     <td style="border:1px solid #999999;"> headers<br> 
      <div>
        它是一个可从中可以提取字段语句的列表 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 4 </td> 
     <td style="border:1px solid #999999;"> parse_row(response,row)<br> 
      <div>
        它接收一个响应，并每一行使用报头键 
      </div> </td> 
    </tr> 
   </tbody> 
  </table> 
  <p style="text-align:justify;"> CSVFeedSpider 示例: </p> 
  <pre>from scrapy.spiders import CSVFeedSpider
from demoproject.items import DemoItem

class DemoSpider(CSVFeedSpider):
    name = "demo"
    allowed_domains = ["www.yiibai.com"]
    start_urls = ["http://www.yiibai.com/feed.csv"]
    delimiter = ";"
    quotechar = "'"
    headers = ["product_title", "product_link", "product_description"]

    def parse_row(self, response, row):
        self.logger.info("This is row: %r", row)

        item = DemoItem()
        item["product_title"] = row["product_title"]
        item["product_link"] = row["product_link"]
        item["product_description"] = row["product_description"]
        return item</pre> 
  <h2> SitemapSpider </h2> 
  <p style="text-align:justify;"> 站点地图(<a target="_blank" href="http://www.sitemaps.org/">sitemap</a>)帮助蜘蛛通过 robots.txt 的定位网址并抓取网站。它有以下类： </p> 
  <pre>class scrapy.spiders.SitemapSpider</pre> 
  <div>
    下面的表显示了SitemapSpider的字段： 
  </div> 
  <table border="1" cellpadding="2" cellspacing="0"> 
   <tbody> 
    <tr> 
     <th style="border:1px solid #999999;"> S.N. </th> 
     <th style="border:1px solid #999999;"> 
      <div>
        字段及说明 
      </div> </th> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 1 </td> 
     <td style="border:1px solid #999999;"> sitemap_urls<br> 
      <div>
        要抓取指向网站地图的URL列表 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 2 </td> 
     <td style="border:1px solid #999999;"> sitemap_rules<br> 
      <div>
        这是一个元组列表&nbsp;(regex,&nbsp;callback)&nbsp;，其中，正则表达式是正则表达式，回调是用来处理的URL匹配的正则表达式 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 3 </td> 
     <td style="border:1px solid #999999;"> sitemap_follow<br> 
      <div>
        这是网站地图的正则表达式的跟踪列表 
      </div> </td> 
    </tr> 
    <tr> 
     <td style="border:1px solid #999999;"> 4 </td> 
     <td style="border:1px solid #999999;"> sitemap_alternate_links<br> 
      <div>
        指定要跟踪一个URL备用链路 
      </div> </td> 
    </tr> 
   </tbody> 
  </table> 
  <p style="text-align:justify;"> SitemapSpider 示例: </p> 
  <div>
    下面是&nbsp;SitemapSpider&nbsp;处理所有的网址： 
  </div> 
  <pre>from scrapy.spiders import SitemapSpider

class DemoSpider(SitemapSpider):
    urls = ["http://www.yiibai.com/sitemap.xml"]

    def parse(self, response):
        # You can scrap items here</pre> 
  <div>
    下面是&nbsp;SitemapSpider&nbsp;处理某些URL与回调： 
  </div> 
  <pre>from scrapy.spiders import SitemapSpider

class DemoSpider(SitemapSpider):
    urls = ["http://www.yiibai.com/sitemap.xml"]
    rules = [
        ("/item/", "parse_item"),
        ("/group/", "parse_group"),
    ]

    def parse_item(self, response):
        # you can scrap item here

    def parse_group(self, response):
        # you can scrap group here &nbsp;</pre> 
  <p style="text-align:justify;"> 下面的代码显示了跟踪站点地图，在 robots.txt 中的网址有&nbsp;/sitemap_company: </p> 
  <pre>from scrapy.spiders import SitemapSpider

class DemoSpider(SitemapSpider):
    urls = ["http://www.yiibai.com/robots.txt"]
    rules = [
        ("/company/", "parse_company"),
    ]
    sitemap_follow = ["/sitemap_company"]

    def parse_company(self, response):
        # you can scrap company here</pre> 
  <div>
    您甚至可以将&nbsp;SitemapSpider&nbsp;与其他网址相结合如下图所示： 
  </div> 
  <pre>from scrapy.spiders import SitemapSpider

class DemoSpider(SitemapSpider):
    urls = ["http://www.yiibai.com/robots.txt"]
    rules = [
        ("/company/", "parse_company"),
    ]

    other_urls = ["http://www.yiibai.com/contact-us"]

    def start_requests(self):
        requests = list(super(DemoSpider, self).start_requests())
        requests += [scrapy.Request(x, self.parse_other) for x in self.other_urls]
        return requests

    def parse_company(self, response):
        # you can scrap company here...
		
    def parse_other(self, response):
        # you can scrap other here...</pre> 
 </div>
 <br>      
</div></body></html>