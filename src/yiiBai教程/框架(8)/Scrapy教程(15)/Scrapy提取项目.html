<html><head><meta charset="utf-8"></meta></head><body><h1 class="article-title" style="text-align:center;">Scrapy提取项目</h1><div style="width:100%;float:left;" class="article-content">   
 <p>从网页中提取数据，Scrapy 使用基于 <a target="_blank" href="https://www.w3.org/TR/xpath/">XPath</a> 和 <a target="_blank" href="https://www.w3.org/TR/selectors/">CSS</a> 表达式的技术叫做选择器。以下是 XPath 表达式的一些例子：</p> 
 <p>/html/head/title:<br>这将选择 HTML 文档中的 &lt;head&gt; 元素中的 &lt;title&gt; 元素。</p> 
 <p>/html/head/title/text(): 这将选择 &lt;title&gt; 元素中的文本。</p> 
 <p>//td: 这将选择所有的 &lt;td&gt; 元素。</p> 
 <p>//div[<a target="_blank" href="https://github.com/class" title="@class" class="at-link">@class</a>=”slice”]: 选择 div 包含一个属性 class=”slice” 的所有元素。</p> 
 <p>选择器有四个基本的方法，如下所示：</p> 
 <table> 
  <thead> 
   <tr> 
    <th>S.N.</th> 
    <th>方法 &amp; 描述</th> 
   </tr> 
  </thead> 
  <tbody> 
   <tr> 
    <td>extract()</td> 
    <td>它返回一个unicode字符串以及所选数据</td> 
   </tr> 
   <tr> 
    <td>re()</td> 
    <td>它返回Unicode字符串列表，当正则表达式被赋予作为参数时提取</td> 
   </tr> 
   <tr> 
    <td>xpath()</td> 
    <td>它返回选择器列表，它代表由指定XPath表达式参数选择的节点。</td> 
   </tr> 
   <tr> 
    <td>css()</td> 
    <td>它返回选择器列表，它代表由指定CSS表达式作为参数所选择的节点。</td> 
   </tr> 
  </tbody> 
 </table> 
 <h2 id="h2--shell-"><a name="在Shell中使用选择器" class="reference-link"></a><span class="header-link octicon octicon-link"></span>在Shell中使用选择器</h2>
 <p>若要演示选择器在内置Scrapy Shell 中，必须要在您的系统中安装 <a target="_blank" href="http://ipython.org/">IPython</a>。 这里最重要的是，在运行时网址应包含Scrapy引号之内; 否则使用的 URL “&amp;” 字符将不起作用。 可以通过在该项目的顶级目录中，使用下面的命令启动一个 shell：</p> 
 <pre><code>scrapy shell "http://www.yiibai.com/scrapy/scrapy_environment.html"
</code></pre>
 <p>shell 执行后结果如下图所示：</p> 
 <pre><code>D:first_scrapy&gt;scrapy shell "http://www.yiibai.com/scrapy/scrapy_environment.html"
2016-10-03 11:45:08 [scrapy] INFO: Scrapy 1.1.2 started (bot: first_scrapy)
2016-10-03 11:45:08 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'first_scrapy.spiders', 'ROBOTSTXT_OBEY': True, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'SPIDER_MODULES': ['first_scrapy.spiders'], 'BOT_NAME': 'first_scrapy', 'LOGSTATS_INTERVAL': 0}
2016-10-03 11:45:08 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-10-03 11:45:08 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-10-03 11:45:08 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-10-03 11:45:08 [scrapy] INFO: Enabled item pipelines:
[]
2016-10-03 11:45:08 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-10-03 11:45:08 [scrapy] INFO: Spider opened
2016-10-03 11:45:09 [scrapy] DEBUG: Crawled (200) &lt;GET http://www.yiibai.com/robots.txt&gt; (referer: None)
2016-10-03 11:45:09 [scrapy] DEBUG: Crawled (200) &lt;GET http://www.yiibai.com/scrapy/scrapy_environment.html&gt; (referer: None)
[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x00000000042E3E80&gt;
[s]   item       {}
[s]   request    &lt;GET http://www.yiibai.com/scrapy/scrapy_environment.html&gt;
[s]   response   &lt;200 http://www.yiibai.com/scrapy/scrapy_environment.html&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x00000000042E3E10&gt;
[s]   spider     &lt;firstSpider 'first' at 0x47f9f28&gt;
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser
&gt;&gt;&gt;
</code></pre>
 <p>当 shell 加载后，可以分别通过使用 response.body 和 response.header 访问主体或头信息。同样，也可以通过使用 response.selector.xpath（）或 response.selector.css（）运行查询的响应结果。</p> 
 <p>例如：</p>   
 <pre><code>&gt;&gt;&gt; response.xpath('//title')
[&lt;Selector xpath='//title' data=u'&lt;title&gt;Scrapyu5b89u88c5 - Scrapyu6559u7a0b&lt;/title&gt;'&gt;]
&gt;&gt;&gt; response.xpath('//title').extract()
[u'&lt;title&gt;Scrapyu5b89u88c5 - Scrapyu6559u7a0b&lt;/title&gt;']
&gt;&gt;&gt; response.xpath('//title/text()')
[&lt;Selector xpath='//title/text()' data=u'Scrapyu5b89u88c5 - Scrapyu6559u7a0b'&gt;]
&gt;&gt;&gt; response.xpath('//title/text()').extract()
[u'Scrapyu5b89u88c5 - Scrapyu6559u7a0b']
&gt;&gt;&gt; response.xpath('//title/text()').extract()
[u'Scrapyu5b89u88c5 - Scrapyu6559u7a0b']
&gt;&gt;&gt; response.xpath('//title/text()').re('(w+):')
[]
&gt;&gt;&gt;
</code></pre>
 <h2 id="h2-u63D0u53D6u6570u636E"><a name="提取数据" class="reference-link"></a><span class="header-link octicon octicon-link"></span>提取数据</h2>
 <p>从一个普通的HTML网站提取数据，查看该网站得到的 XPath 的源代码。检测后，可以看到数据将在UL标签，并选择 li 标签中的 元素。</p> 
 <p>代码的下面行显示了不同类型的数据的提取：</p> 
 <p>选择 li 标签内的数据：</p> 
 <pre><code>response.xpath('//ul/li')
</code></pre>
 <p>对于选择描述：</p> 
 <pre><code>response.xpath('//ul/li/text()').extract()
</code></pre>
 <p>对于选择网站标题：</p> 
 <pre><code>response.xpath('//ul/li/a/text()').extract()
</code></pre>
 <p>对于选择网站的链接：</p> 
 <pre><code>response.xpath('//ul/li/a/@href').extract()
</code></pre>
 <p>下面的代码用于演示上述提取的用法：</p> 
 <pre><code>import scrapy

class MyprojectSpider(scrapy.Spider):
    name = "project"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def parse(self, response):
        for sel in response.xpath('//ul/li'):
            title = sel.xpath('a/text()').extract()
            link = sel.xpath('a/@href').extract()
            desc = sel.xpath('text()').extract()
            print title, link, desc
</code></pre>
 <br>      
</div></body></html>