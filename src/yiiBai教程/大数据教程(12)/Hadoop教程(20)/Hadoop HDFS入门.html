<html><head><meta charset="utf-8"></meta></head><body><h1 class="article-title" style="text-align:center;">Hadoop HDFS入门</h1><div style="width:100%;float:left;" class="article-content">   Hadoop 附带了一个名为 HDFS(Hadoop分布式文件系统)的分布式文件系统，基于 Hadoop 的应用程序使用 HDFS 。HDFS 是专为存储超大数据文件，运行在集群的商品硬件上。它是容错的，可伸缩的，并且非常易于扩展。 
 <div>
   &nbsp; &nbsp; 你知道吗? &nbsp;当数据超过一个单个物理机器上存储的容量，除以跨独立机器数。管理跨越机器的网络存储特定操作被称为分布式文件系统。 
  <div> 
   <p> HDFS集群主要由 NameNode 管理文件系统 Metadata 和 DataNodes 存储的实际数据。</p> 
   <ul> 
    <li> 
     <div> 
      <strong>NameNode:</strong>&nbsp;NameNode可以被认为是系统的主站。它维护所有系统中存在的文件和目录的文件系统树和元数据 。 两个文件：“命名空间映像“和”编辑日志“是用来存储元数据信息。Namenode 有所有包含数据块为一个给定的文件中的数据节点的知识，但是不存储块的位置持续。从数据节点在系统每次启动时信息重构一次。
     </div> </li> 
    <li> 
     <div> 
      <strong>DataNode :</strong>&nbsp;DataNodes作为从机，每台机器位于一个集群中，并提供实际的存储. 它负责为客户读写请求服务。
     </div> </li> 
   </ul> 
   <p> HDFS中的读/写操作运行在块级。HDFS数据文件被分成块大小的块，这是作为独立的单元存储。默认块大小为64 MB。</p> 
   <p> HDFS操作上是数据复制的概念，其中在数据块的多个副本被创建，分布在整个节点的群集以使在节点故障的情况下数据的高可用性。</p> 
   <p> 注： &nbsp;在HDFS的文件，比单个块小，不占用块的全部存储。</p> 
   <h3> 在HDFS读操作</h3> 
   <p> 数据读取请求将由 HDFS，NameNode和DataNode来服务。让我们把读取器叫 “客户”。下图描绘了文件的读取操作在 Hadoop 中。<br> <img alt="" src="/uploads/allimg/201509/1-1509131002351I.png" style="width: 641px; height: 453px;"></p> 
   <ol> 
    <li> 客户端启动通过调用文件系统对象的 open() 方法读取请求; 它是&nbsp;<strong>DistributedFileSystem&nbsp;</strong>类型的对象。</li> 
    <li> 此对象使用 RPC 连接到 namenode 并获取的元数据信息，如该文件的块的位置。 请注意，这些地址是文件的前几个块。</li> 
    <li> 响应该元数据请求，具有该块副本的 DataNodes 地址被返回。</li> 
    <li> 
     <div>
       一旦接收到 DataNodes 的地址，FSDataInputStream 类型的一个对象被返回到客户端。&nbsp;FSDataInputStream 包含 DFSInputStream 这需要处理交互 DataNode 和 NameNode。在上图所示的步骤4，客户端调用 read() 方法，这将导致 DFSInputStream 建立与第一个 DataNode 文件的第一个块连接。
     </div> </li> 
    <li> 
     <div>
       以数据流的形式读取数据，其中客户端多次调用 “read() ” 方法。 read() 操作这个过程一直持续，直到它到达块结束位置。
     </div> </li> 
    <li> 一旦到模块的结尾，DFSInputStream 关闭连接，移动定位到下一个 DataNode 的下一个块</li> 
    <li> 一旦客户端已读取完成后，它会调用 close()方法。</li> 
   </ol> 
   <h3> HDFS写操作</h3> 
   <p> 在本节中，我们将了解如何通过的文件将数据写入到 HDFS。<br> <img alt="" src="/uploads/allimg/201509/1-15091310030Xa.png" style="width: 642px; height: 480px;"></p> 
   <ol> 
    <li> 客户端通过调用 DistributedFileSystem对象的 create() 方法创建一个新的文件，并开始写操作 - 在上面的图中的步骤1</li> 
    <li> DistributedFileSystem对象使用 RPC 调用连接到 NameNode，并启动新的文件创建。但是，此文件创建操作不与文件任何块相关联。NameNode 的责任是验证文件(其正被创建的)不存在，并且客户端具有正确权限来创建新文件。如果文件已经存在，或者客户端不具有足够的权限来创建一个新的文件，则抛出 IOException 到客户端。否则操作成功，并且该文件新的记录是由 NameNode 创建。</li> 
    <li> 一旦 NameNode 创建一条新的记录，返回FSDataOutputStream 类型的一个对象到客户端。客户端使用它来写入数据到 HDFS。数据写入方法被调用(图中的步骤3)。</li> 
    <li> FSDataOutputStream包含DFSOutputStream对象，它使用 DataNodes 和 NameNode 通信后查找。当客户机继续写入数据，DFSOutputStream&nbsp;继续创建这个数据包。这些数据包连接排队到一个队列被称为&nbsp;DataQueue</li> 
    <li> 还有一个名为 DataStreamer 组件，用于消耗DataQueue。DataStreamer 也要求 NameNode 分配新的块，拣选 DataNodes 用于复制。</li> 
    <li> 现在，复制过程始于使用 DataNodes 创建一个管道。 在我们的例子中，选择了复制水平3，因此有 3 个 DataNodes 管道。</li> 
    <li> 所述 DataStreamer 注入包分成到第一个 DataNode 的管道中。</li> 
    <li> 在每个 DataNode 的管道中存储数据包接收并同样转发在第二个 DataNode 的管道中。</li> 
    <li> 另一个队列，“Ack Queue”是由 DFSOutputStream 保持存储，它们是 DataNodes 等待确认的数据包。</li> 
    <li> 一旦确认在队列中的分组从所有 DataNodes 已接收在管道，它从 'Ack Queue' 删除。在任何 DataNode 发生故障时，从队列中的包重新用于操作。</li> 
    <li> 在客户端的数据写入完成后，它会调用close()方法(第9步图中)，调用close()结果进入到清理缓存剩余数据包到管道之后等待确认。</li> 
    <li> 一旦收到最终确认，NameNode 连接告诉它该文件的写操作完成。</li> 
   </ol> 
   <h3> 使用JAVA API访问HDFS</h3> 
   <p> 在本节中，我们来了解 Java 接口并用它们来访问Hadoop的文件系统。</p> 
   <p> 为了使用编程方式与 Hadoop 文件系统进行交互，Hadoop 提供多种 Java 类。<strong><em>org.apache.hadoop.fs </em></strong>包中包含操纵 Hadoop 文件系统中的文件类工具。这些操作包括，打开，读取，写入，和关闭。实际上，对于 Hadoop 文件 API 是通用的，可以扩展到 HDFS 的其他文件系统交互。</p> 
   <p> <strong>编程从 HDFS 读取文件</strong></p> 
   <p> java.net.URL 对象是用于读取文件的内容。首先，我们需要让 Java 识别 Hadoop 的 HDFS URL架构。这是通过调用 URL 对象的 <strong>setURLStreamHandlerFactory</strong>方法和&nbsp;<strong>FsUrlStreamHandlerFactory </strong>的一个实例琮传递给它。此方法只需要执行一次在每个JVM，因此，它被封闭在一个静态块中。</p> 
   <p> 示例代码</p> 
   <div style="color: rgb(52, 52, 52); 'Droid Sans'; font-size: 17px;"> 
    <div style="font-size: 1em !important;"> 
     <table border="1" cellpadding="2" cellspacing="0" style="border-collapse: collapse; margin-top: 10px; margin-bottom: 10px; font-size: 1em !important; Consolas, 'Bitstream Vera Sans Mono', 'Courier New', Courier, monospace !important; background-color: inherit;"> 
      <tbody style="background-color: inherit; font-size: 1em !important;"> 
       <tr style="background-color: inherit; font-size: 1em !important;"> 
        <td> &nbsp;</td> 
        <td> 
         <div> 
          <div>
            publicclassURLCat {
          </div> 
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;static{
          </div> 
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;URL.setURLStreamHandlerFactory(newFsUrlStreamHandlerFactory());
          </div> 
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;}
          </div> 
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;publicstaticvoidmain(String[] args)&nbsp;throwsException {
          </div> 
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;InputStream in =&nbsp;null;
          </div> 
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try{
          </div> 
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in =&nbsp;newURL(args[0]).openStream();
          </div> 
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IOUtils.copyBytes(in, System.out,&nbsp;4096,&nbsp;false);
          </div> 
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;finally{
          </div> 
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IOUtils.closeStream(in);
          </div> 
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
          </div> 
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;}
          </div> 
          <div>
            }
          </div> 
         </div> </td> 
       </tr> 
      </tbody> 
     </table> 
    </div> 
   </div> 
   <p> 这段代码用于打开和读取文件的内容。HDFS文件的路径作为命令行参数传递给该程序。</p> 
   <h3> 使用命令行界面访问HDFS</h3> 
   <p> 这是与 HDFS 交互的最简单的方法之一。 命令行接口支持对文件系统操作，例如：如读取文件，创建目录，移动文件，删除数据，并列出目录。</p> 
   <p> 可以执行&nbsp;'$HADOOP_HOME/bin/hdfs dfs -help'&nbsp;来获得每一个命令的详细帮助。这里,&nbsp;'dfs'&nbsp;HDFS是一个shell命令，它支持多个子命令。首先要启动 Haddop 服务(使用 hduser_用户)，执行命令如下：</p>   
   <pre style="'Courier New', Courier, monospace; color: rgb(51, 51, 51); font-size: 14px; background-color: rgb(245, 245, 245);">
hduser_@ubuntu:~$ su hduser_
hduser_@ubuntu:~$ $HADOOP_HOME/sbin/start-dfs.sh
hduser_@ubuntu:~$ $HADOOP_HOME/sbin/start-yarn.sh</pre> 
   <p> 一些广泛使用的命令的列表如下</p> 
   <p> <strong>1. 从本地文件系统复制文件到 HDFS</strong></p> 
   <pre style="'Courier New', Courier, monospace; color: rgb(51, 51, 51); font-size: 14px; background-color: rgb(245, 245, 245);">
hduser_@ubuntu:~$ $HADOOP_HOME/bin/hdfs dfs -copyFromLocal temp.txt /</pre> 
   <p> 此命令将文件从本地文件系统拷贝 temp.txt 文件到 HDFS。</p> 
   <p> <strong>2. 我们可以通过以下命令列出一个目录下存在的文件<em>&nbsp;-ls</em></strong></p> 
   <pre style="'Courier New', Courier, monospace; color: rgb(51, 51, 51); font-size: 14px; background-color: rgb(245, 245, 245);">
hduser_@ubuntu:~$ $HADOOP_HOME/bin/hdfs dfs -ls /</pre> 
   <p style="margin: 5px 0px; color: rgb(52, 52, 52); 'Droid Sans'; font-size: 17px;"> <img data-attr-org-img-file="file:///D:/Program%20Files/qq24B39E2329F08D9F4C3146A571A415EF/1a4e9147d4b54475864762f2044ba4dd/clipboard.png" data-attr-org-src-id="63F3CEB9EDA44BD5BE7284B02D2E1636" data-media-type="image" src="file:///D:/Program%20Files/qq24B39E2329F08D9F4C3146A571A415EF/1a4e9147d4b54475864762f2044ba4dd/clipboard.png" style="display: inline-block; margin-top: 8px; max-width: 800px; background-color: inherit;"><img alt="" src="/uploads/allimg/201509/1-150913100333403.png" style="width: 673px; height: 75px;"></p> 
   <p> 我们可以看到一个文件 'temp.txt“(之前复制)被列在”/“目录。</p> 
   <p> <strong>3. 以下命令将文件从 HDFS 拷贝到本地文件系统</strong></p> 
   <pre style="'Courier New', Courier, monospace; color: rgb(51, 51, 51); font-size: 14px; background-color: rgb(245, 245, 245);">
hduser_@ubuntu:~$ $HADOOP_HOME/bin/hdfs dfs -copyToLocal /temp.txt</pre> 
   <p> 我们可以看到 temp.txt 已经复制到本地文件系统。</p> 
   <p> <strong>4. 以下命令用来创建新的目录</strong></p> 
   <pre style="'Courier New', Courier, monospace; color: rgb(51, 51, 51); font-size: 14px; background-color: rgb(245, 245, 245);">
hduser_@ubuntu:~$ $HADOOP_HOME/bin/hdfs dfs -mkdir /mydirectory</pre> 
   <p> <a target="_blank" href="http://cdn.guru99.com/images/Big_Data/061114_0923_LearnHDFSAB6.png"><img data-attr-org-img-file="file:///D:/Program%20Files/qq24B39E2329F08D9F4C3146A571A415EF/1995ed7b1d674966860f1435713c0c3e/learnhdfsab6.png" data-attr-org-src-id="F53049EFF20F466A9E119B6505DCF7EC" data-media-type="image" src="file:///D:/Program%20Files/qq24B39E2329F08D9F4C3146A571A415EF/1995ed7b1d674966860f1435713c0c3e/learnhdfsab6.png"></a>接下来检查是否已经建立了目录。现在，应该知道怎么做了吧？</p> 
  </div> 
 </div> 
 <br> 
 <br>      
</div></body></html>